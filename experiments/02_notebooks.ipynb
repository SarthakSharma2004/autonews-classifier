{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ed9e808",
   "metadata": {},
   "source": [
    "## PREPROCESSING AND PREPARING DATA FOR MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1febdac",
   "metadata": {},
   "source": [
    "<!-- -- -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b76c369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "import os \n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c6e795d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Musicians to tackle US red tape\\n\\nMusicians' ...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>musicians to tackle us red tape musicians grou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U2's desire to be number one\\n\\nU2, who have w...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>u2s desire to be number one u2 who have won th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rocker Doherty in on-stage fight\\n\\nRock singe...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>rocker doherty in onstage fight rock singer pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Snicket tops US box office chart\\n\\nThe film a...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>snicket tops us box office chart the film adap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ocean's Twelve raids box office\\n\\nOcean's Twe...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>oceans twelve raids box office oceans twelve t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text          label  \\\n",
       "0  Musicians to tackle US red tape\\n\\nMusicians' ...  entertainment   \n",
       "1  U2's desire to be number one\\n\\nU2, who have w...  entertainment   \n",
       "2  Rocker Doherty in on-stage fight\\n\\nRock singe...  entertainment   \n",
       "3  Snicket tops US box office chart\\n\\nThe film a...  entertainment   \n",
       "4  Ocean's Twelve raids box office\\n\\nOcean's Twe...  entertainment   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  musicians to tackle us red tape musicians grou...  \n",
       "1  u2s desire to be number one u2 who have won th...  \n",
       "2  rocker doherty in onstage fight rock singer pe...  \n",
       "3  snicket tops us box office chart the film adap...  \n",
       "4  oceans twelve raids box office oceans twelve t...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/Users/sarthaksharna/AutoNews/data/cleaned/cleaned_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dc8358",
   "metadata": {},
   "source": [
    "<!-- --- -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be4903fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTIFACTS_PATH = \"../artifacts\"\n",
    "os.makedirs(ARTIFACTS_PATH, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7516a024",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 5000 \n",
    "MAX_LEN = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a6587f",
   "metadata": {},
   "source": [
    "<!-- -- -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a029590a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1701,), (426,), (1701,), (426,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[\"cleaned_text\"]\n",
    "y = df[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9873d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "sport            404\n",
       "business         402\n",
       "politics         322\n",
       "entertainment    295\n",
       "tech             278\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ed9757",
   "metadata": {},
   "source": [
    "<!-- ---- -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af95acb",
   "metadata": {},
   "source": [
    "SAVING LABEL ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c598d64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoder(y_train, y_test):\n",
    "    \"\"\"\n",
    "    This function encodes the labels using LabelEncoder()\n",
    "    \"\"\"\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y_train = le.fit_transform(y_train)\n",
    "    y_test = le.transform(y_test)\n",
    "    return y_train, y_test, le\n",
    "\n",
    "y_train, y_test, le = label_encoder(y_train, y_test)\n",
    "y_train, y_test\n",
    "\n",
    "with open(os.path.join(ARTIFACTS_PATH, \"label_encoder.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(le, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bdf2865a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Encoded labels saved!\n"
     ]
    }
   ],
   "source": [
    "np.save(os.path.join(ARTIFACTS_PATH, \"y_train.npy\"), y_train)\n",
    "np.save(os.path.join(ARTIFACTS_PATH, \"y_test.npy\"), y_test)\n",
    "\n",
    "print(\"✓ Encoded labels saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68973815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class mapping: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'business': 0, 'entertainment': 1, 'politics': 2, 'sport': 3, 'tech': 4}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"class mapping: \")\n",
    "label_map = {label: idx for idx, label in enumerate(le.classes_)}\n",
    "label_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cb18af",
   "metadata": {},
   "source": [
    "<!-- --- -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5a1f12",
   "metadata": {},
   "source": [
    "SAVING TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9191fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(X_train, X_test):\n",
    "    \"\"\"This function tokenizes the text data\"\"\"\n",
    "\n",
    "    tokenizer = Tokenizer(num_words = VOCAB_SIZE, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "    X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "    return X_train_seq, X_test_seq, tokenizer\n",
    "\n",
    "\n",
    "X_train_seq, X_test_seq, tokenizer = tokenization(X_train, X_test)\n",
    "\n",
    "with open(os.path.join(ARTIFACTS_PATH, \"tokenizer.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d92bd72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whitehall cuts ahead of target thousands of civil service jobs have already been cut or moved out of london as part of a major costcutting drive chancellor gordon brown said 12500 jobs had gone while 7800 were being moved out of the south east he plans to axe 104000 jobs to free up money for education health defence housing and overseas aid unions oppose the plans but mr brown said 2bn savings had already been made and more jobs had been cut than had been expected at this stage a further 200 jobs at the department of the environment food and rural affairs have been earmarked to be cut at the department for work and pensions 30000 jobs are to go 560 will be lost by the end of the month at the department of trade and industry and 400 are to go at the inland revenue and customs in his budget statement the chancellor said the first 12500 civil service jobs had been cut on target about 4300 civil servants will leave london and the south east by the end of march 2005 and there are plans to relocate another 3500 of those 300 department of health posts will go to yorkshire while hundreds at the department of culture will move to birmingham and newcastle mr brown also announced plans to merge 35 agencies into nine described by one civil service union as a bonfire of the quangos mark serwotka the leader of the public and commercial services union said in last years budget we had the day of the long knifes as the chancellor kicked off the crude game of who could cut the most civil service jobs between the government and the tories there was a time when the only worry thousands of hard working civil and public servants had on budget day was whether petrol or taxes would go up nowadays the worry is whether they will have a job by the end of it he said mr brown had made welcome announcements on closing tax loopholes and extending the new deal while cutting the very people who deliver them the treasury also announced plans to reduce the number of public sector workers on sick leave with a new system of checks and tougher measures against those suspected of abusing the system\n",
      "[1, 784, 368, 4, 847, 1127, 4, 883, 200, 551, 20, 184, 40, 304, 53, 1128, 56, 4, 212, 18, 152, 4, 6, 393, 1, 796, 387, 738, 210, 12, 1, 551, 36, 841, 98, 1, 41, 89, 1128, 56, 4, 2, 371, 1006, 15, 170, 3, 1, 1, 551, 3, 518, 47, 198, 8, 677, 552, 884, 1240, 5, 1832, 547, 2129, 4688, 2, 170, 22, 30, 210, 12, 3417, 1494, 36, 184, 40, 91, 5, 42, 551, 36, 40, 304, 55, 36, 40, 174, 21, 35, 755, 6, 355, 1987, 551, 21, 2, 899, 4, 2, 1666, 1942, 5, 2382, 1376, 20, 40, 1, 3, 16, 304, 21, 2, 899, 8, 148, 5, 1353, 3869, 551, 25, 3, 136, 1, 24, 16, 281, 23, 2, 167, 4, 2, 271, 21, 2, 899, 4, 398, 5, 229, 5, 3870, 25, 3, 136, 21, 2, 1, 1450, 5, 4917, 7, 29, 394, 446, 2, 387, 12, 2, 65, 1, 883, 200, 551, 36, 40, 304, 11, 847, 49, 1, 883, 3182, 24, 961, 212, 5, 2, 371, 1006, 23, 2, 167, 4, 440, 206, 5, 52, 25, 170, 3, 1, 226, 1, 4, 139, 2877, 899, 4, 552, 4479, 24, 136, 3, 4689, 98, 1865, 21, 2, 899, 4, 1451, 24, 243, 3, 1452, 5, 1330, 30, 210, 43, 405, 170, 3, 3871, 1901, 2213, 72, 982, 1160, 23, 50, 883, 200, 610, 18, 6, 1, 4, 2, 3703, 456, 1, 2, 305, 4, 2, 168, 5, 1252, 217, 610, 12, 7, 63, 66, 394, 34, 36, 2, 228, 4, 2, 316, 1, 18, 2, 387, 2973, 149, 2, 1833, 88, 4, 44, 58, 304, 2, 112, 883, 200, 551, 141, 2, 76, 5, 2, 407, 52, 14, 6, 68, 64, 2, 81, 2493, 1127, 4, 406, 384, 883, 5, 168, 3182, 36, 11, 394, 228, 14, 310, 1, 53, 706, 37, 136, 47, 1, 2, 2493, 9, 310, 32, 24, 20, 6, 492, 23, 2, 167, 4, 13, 15, 12, 30, 210, 36, 91, 2690, 1, 11, 2691, 208, 1, 5, 4039, 2, 45, 203, 98, 1690, 2, 92, 48, 44, 1426, 80, 2, 1724, 43, 405, 170, 3, 1525, 2, 105, 4, 168, 929, 1253, 11, 1, 961, 17, 6, 45, 227, 4, 4918, 5, 4480, 1051, 78, 139, 3183, 4, 1, 2, 227]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.iloc[0])\n",
    "print(X_train_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "194c449e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words in corpus: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30808"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Total unique words in corpus: \")\n",
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b9ef85bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index assigned to '<OOV>' token: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Index assigned to '<OOV>' token: \")\n",
    "tokenizer.word_index[\"<OOV>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1bc398b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most frequent words: \n",
      "1: '<OOV>'\n",
      "2: 'the'\n",
      "3: 'to'\n",
      "4: 'of'\n",
      "5: 'and'\n",
      "6: 'a'\n",
      "7: 'in'\n",
      "8: 'for'\n",
      "9: 'is'\n",
      "10: 'that'\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 10 most frequent words: \")\n",
    "for i in range(1, 11):\n",
    "    print(f\"{i}: '{tokenizer.index_word[i]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208376dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words: 30808\n",
      "Words used in vocab: 5000\n",
      "Words treated as OOV: 25808\n"
     ]
    }
   ],
   "source": [
    "total_words = len(tokenizer.word_index)\n",
    "\n",
    "used_words = VOCAB_SIZE\n",
    "\n",
    "oov_words = total_words - used_words\n",
    "\n",
    "print(f\"Total unique words: {total_words}\")\n",
    "print(f\"Words used in vocab: {used_words}\")\n",
    "print(f\"Words treated as OOV: {oov_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e75cf98",
   "metadata": {},
   "source": [
    "<!-- --- -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841f92a6",
   "metadata": {},
   "source": [
    "SAVING PADDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfec5ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq(X_train_seq, X_test_seq):\n",
    "    \"\"\"This function pads the sequences with max length of 200\"\"\"\n",
    "\n",
    "    X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "    X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "    return X_train_pad, X_test_pad \n",
    "\n",
    "\n",
    "X_train_pad, X_test_pad = pad_seq(X_train_seq, X_test_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5cbd58aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Padded sequences saved!\n"
     ]
    }
   ],
   "source": [
    "np.save(os.path.join(ARTIFACTS_PATH, \"X_train_pad.npy\"), X_train_pad)\n",
    "np.save(os.path.join(ARTIFACTS_PATH, \"X_test_pad.npy\"), X_test_pad)\n",
    "\n",
    "print(\"✓ Padded sequences saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ab6f4efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1701, 200), (426, 200))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pad.shape, X_test_pad.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97f2494",
   "metadata": {},
   "source": [
    "<!-- ----- -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
